<html>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <head>
    <title>Chaofeng</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/c/font_1155135_w0owrxa96yp.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="stylesheet" type="text/css" href="css/fonts.css">
</head>

<body>
	<div class="main-container">
    <!-------------------------------  Introduction -------------------------------------------->
		<div class="content-container font-hei">
        <div class="hflex-container" id="profile">
				<img src="images/me.jpg">
				<div>
          <br>
					<b style="font-size:25px"><em>CHEN Chaofeng</em></b>
					<b class="font-song" style="font-size:25px">(陈超锋)</b>
					<!-- <p>B.Eng. HUST ; Ph.D HKU<br/> -->
					<!-- <a href="http://www.visionlab.cs.hku.hk/">Computer Vision Lab</a>, -->
	        <!-- <a href="http://www.cs.hku.hk">Dept. of Computer Science</a>, -->
	        <!-- <a href="http://www.hku.hk">The University of Hong Kong</a>. -->
					<br><br>
					<table>
				        <tbody>
                  <tr>
                    <td>
                      Research Fellow @ S-Lab, NTU
                    </td>
                  </tr>
                  <tr>
                    <td><br>Ph.D @ <a href="https://www.hku.hk/" target="_blank">HKU</a>; B.Eng. @ <a href="https://www.hust.edu.cn/" target="_blank">HUST</a></td>
                  </tr>
				        	<tr>
				        		<!-- <td width="60px"><b>Email</b></td> -->
				        		<td><i class="fa fa-envelope" style="color:black;font-size:22;"></i> chaofenghust [at] gmail.com</td>
				        	</tr>
					        <!-- <tr>
					        	<td style="vertical-align:text-top;"><b>Office</b></td>
					        	<td>CB411, Chow Yei Ching Building, <br>
					        		The University of Hong Kong,
									Pokfulam Road, Hong Kong</td> -->
							<!-- </tr> -->
							<tr>
								<td colspan="2">
									<br>
									<a href="https://github.com/chaofengc" target="_blank"><i class="fa-brands fa-github" style="color:black;font-size:22;"></i>&nbsp;Github</a>
									<a href="files/resume-cfchen-en.pdf" target="_blank"><i class="iconfont icon-jianli" style="color:black;font-size:20;"></i>&nbsp;CV</a>
                  <a href="https://www.linkedin.com/in/%E9%99%88-%E8%B6%85%E9%94%8B-aa96518b/" target="_blank"><i class="fa-brands fa-linkedin" style="color:black;font-size:22;"></i>&nbsp;LinkedIn</a>
                  <a href="https://scholar.google.com/citations?user=lxiqnI0AAAAJ&hl=en" target="_blank"><i class="iconfont icon-gscholar" style="color:black;font-size:20;"></i>&nbsp;Google Scholar</a>
									<a href="https://www.zhihu.com/people/fly-cfchen" target="_blank"><i class="iconfont icon-zhihu1" style="color:black;font-size:22"></i></a>
                </td>
								</td>
							</tr>
				        </tbody>
		        	</table>
			    </div>
        </div>
		<!-- </div> -->
    <!-------------------------------  About me -------------------------------------------->
		<!-- <div class="content-container"> -->
			<h2>About Me</h2>
			<p>I am currently a postdoctoral research fellow at S-Lab in Nanyang Technological University, working with <a href="https://personal.ntu.edu.sg/wslin/Home.html" target="_blank">Prof. Weisi Lin</a>. I received my Ph.D. degree from <a href="https://www.cs.hku.hk/" target="_blank">Dept. of Computer Science at the University of Hong Kong</a> in January 2021. I did my Ph.D. research at <a href="http://www.visionlab.cs.hku.hk/">Computer Vision Lab in HKU</a>, advised by <a href="http://i.cs.hku.hk/~kykwong/" target="_blank">Dr. Kenneth K.Y. Wong</a>. Prior to studying at HKU, I received my B.Eng. from <a href="http://www.hust.edu.cn" target="_blank">Huazhong University of Science and Technology</a>.</p>

			<p>My interests are centered around Computer Vision and Deep Learning. Current research topics include:</p>
          <ul>
            <li>Low level vision, including image quality assessment, restoration and enhancement.</li>
            <li>Image-to-image translation</li>
            <li>3D-aware image synthesis/rendering/editing</li>
            <li>Face related tasks</li>
					</ul>
    <!-- </div> -->
    <!-------------------------------  Education -------------------------------------------->
    <h2>News</h2>
    <div class="news">
    <ul class="news">
        <li style="color: red;">⭐ Use <code style="color: black;background-color: yellow;">pip install pyiqa</code> to try our PyTorch toolbox for Image Quality Assessment<a href="https://github.com/chaofengc/IQA-PyTorch" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/Chaofengc/IQA-PyTorch?style=social"></a>.</li>
        <li>2023-09: We release <a href="https://github.com/VQAssessment/Q-Bench" target="_blank">Q-Bench</a>, a systematic benchmark for multi-modality LLMs (MLLMs) on low-level vision and visual quality assessment.</li>
        <li>2023-09: Extension of <a href="https://github.com/VQAssessment/FAST-VQA-and-FasterVQA" target="_blank">FAST-VQA (FasterVQA)</a> get accepted by TPAMI.</li>
        <li>2023-07: One paper about video quality assessment is accepted by ACM MM 2023.</li>
        <li>2023-07: One paper about video quality assessment is accepted by ICCV 2023.</li>
        <li>2023-03: One paper about video quality assessment is accepted by ICME 2023.</li>
        <li>2023-02: One paper about video quality assessment is accepted by TCSVT 2023.</li>
        <li>2022-12: One paper about video prediction is accepted by AAAI 2023.</li>
        <li>2022-11: Our research team, <a href="https://github.com/QualityAssessment" target="_blank">NTU Visual Quality Assessment Group</a> is created, which aims to build efficient and explainable Visual Quality Assessment approaches.</li>
        <li>2022-09: One paper is accepted by NeurIPS 2022.</li>
        <li>2022-07: Three papers have been accepted by ECCV2022.</li>
        <li>2022-06: Two papers, including <a href="https://arxiv.org/abs/2202.13142" target="_blank">QuanTexSR (renamed as FeMaSR)</a> have been accepted by ACM MM2022.</li>
        <li>2022-06: One paper, <a href="https://arxiv.org/abs/2202.07358" target="_blank">FFRNet</a> about masked face recognition has been accepted by ICIP2022.</li>
        <li>2022-03: We release our work about blind image resolution, <a href="https://arxiv.org/abs/2202.13142" target="_blank">QuanTexSR</a>, together with the codes in <a href="https://github.com/chaofengc/QuanTexSR" target="_blank">Github</a>.</li>
        <li>2022-02: We release a PyTorch toolbox for IQA <a href="https://github.com/chaofengc/IQA-PyTorch" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/Chaofengc/IQA-PyTorch?style=social"></a> as well as a comprehensive survey <a href="https://github.com/chaofengc/Awesome-Image-Quality-Assessment" target="_blank"> <img class="imgbadge" src="https://img.shields.io/github/stars/Chaofengc/Awesome-Image-Quality-Assessment?style=social"></a>.</li>
        <li>2021-07: One paper about HDR video reconstruction is accepted by ICCV 2021.</li>
        <li>2021-03: Our paper <a href="https://github.com/chaofengc/PSFRGAN" target="_blank">PSFR-GAN</a> about face SR has been accepted by CVPR2021.</li>
        <li>2020-11: Our paper <a href="https://github.com/chaofengc/Face-SPARNet" target="_blank">SPARNet</a> about face SR has been accepted by TIP2020.</li>
    </ul>
    </div>

    <!-------------------------------  Experience -------------------------------------------->
    <!-- <div class="content-container" display="flex"> -->
		<h2>Experience</h2>
      <div class="experience">
      <table>
        <tr>
          <td>Sep 2021 - Present</td>
          <td>Postdoctoral research fellow at S-Lab in NTU, working with <a href="https://personal.ntu.edu.sg/wslin/Home.html" target="_blank">Prof. Weisi Lin</a></td>
        </tr>
        <tr>
          <td>Mar 2021 - Aug 2021</td>
          <td>Research Assistant at <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/index.html" target="_blank">GAP Lab CUHKSZ</a>, worked with <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/index.html" target="_blank">Dr. Xiaoguang Han</a></td>
        </tr>
        <tr>
          <td>Nov 2019 - Mar 2021</td>
          <td>Research Intern at Alibaba DAMO Academy, worked with <a href="https://www4.comp.polyu.edu.hk/~cslzhang/" target="_blank">Prof. Lei Zhang</a> and <a href="https://csxmli2016.github.io/", target="_blank">Dr. Xiaoming Li</a> </td>
        </tr>
        <tr>
          <td>May 2019 - Oct 2019</td>
          <td>Research Visitor at VLLab UC Merced, worked with <a href="https://faculty.ucmerced.edu/mhyang/" target="_blank">Prof. Ming-Hsuan Yang</a></td>
        </tr>
          <td>Jun 2018 - Mar 2019</td>
          <td>Research Intern at Tencent AI Lab, worked with <a href="https://dblp.uni-trier.de/pers/l/Li:Zhifeng.html" target="_blank">Prof. Zhifeng Li</a> and <a href="https://www.cise.ufl.edu/~dihong/", target="_blank">Dr. Dihong Gong</td>
        <tr>
        </tr>
      </table>
    </div>
    <!-------------------------------  Publications -------------------------------------------->
    <!-- <div class="content-container"> -->
    <br>  
    <h2 style="display:inline">Publications</h2> <span style="font-size: 11pt;">( <sup>*</sup> equal contribution.)</span>
        <h3>Preprints</h3> 
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/qbench.png">
                </td>
                <td>
                  <p class="title">Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision</p>
                </td>
              </tr>
              <tr>
          		  <td>
         		      <p class="authors">Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin</p>
                </td>
              </tr>
              <tr>
                <td>
                  <p class="venue">arXiv, 2023</p>
                </td> 
              </tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2309.14181" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/VQAssessment/Q-Bench" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/VQAssessment/Q-Bench?style=social"></a>
              </td></tr>
            </table>
          </div>
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
                  <img src="images/arXiv2023_topiq.png">
                </td>
                <td>
                  <p class="title">TOPIQ: A Top-down Approach from Semantics to Distortions for Image Quality Assessment</p>
                </td>
              </tr>
              <tr>
          		  <td>
                  <p class="authors"><span class="author_me">Chaofeng Chen</span>, Jiadi Mo, Jingwen Hou, HaoNing Wu, Liang Liao, Wenxiu Sun, Qiong Yan, Weisi Lin</p>
                </td>
              </tr>
              <tr>
                <td>
                  <p class="venue">arXiv, 2023</p>
                </td> 
              </tr>
              <tr><td>
                <a href="https://arxiv.org/abs/2308.03060" target="_blank"><img class="imgbadge arxivbadge"></a>
                <a href="https://github.com/chaofengc/IQA-PyTorch" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/chaofengc/IQA-PyTorch?style=social"></a>
              </td></tr>
            </table>
          </div> 
          

          <!-- <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
                </td>
                <td>
                </td>
              </tr>
              <tr><td>
              </td></tr>
              <tr><td>
              </td></tr>
              <tr><td>
              </td></tr>
            </table>
          </div>  -->

        <h3>Conference Proceedings</h3> 
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/ICME2023_buona_vista.png">
                </td>
                <td>
                  <p class="title">Exploring Opinion-Unaware Video Quality Assessment with Semantic Affinity Criterion</p>
                </td>
              </tr>
              <tr>
          		  <td>
         		      <p class="authors">Haoning Wu, Liang Liao, Jingwen Hou, <span class="author_me">Chaofeng Chen</span>, Erli Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin</p>
                </td>
              </tr>
              <tr>
                <td>
                  <p class="venue">IEEE International Conference on Multimedia and Expo (ICME), 2023</p>
                </td> 
              </tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2302.13269" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/VQAssessment/BVQI" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/VQAssessment/bvqi?style=social"></a>
              </td></tr>
            </table>
          </div>
        
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/AAAI2023_MIMO.png">
                </td>
                <td>
                  <p class="title">MIMO Is All You Need: A Strong Multi-In-Multi-Out Baseline for Video Prediction</p>
                </td>
              </tr>
              <tr><td>
         		    <p class="authors">Shuliang Ning<sup>*</sup>, Mengcheng Lan<sup>*</sup>, Yanran Li, <span class="author_me">Chaofeng Chen</span>, Qian Chen, Xunlai Chen, Xiaoguang Han, Shuguang Cui</p>
              </td></tr>
              <tr><td>
                <p class="venue">Association for the Advancement of Artificial Intelligence (AAAI), 2023</p>
              </td></tr>
              <tr><td>
                <a href="https://arxiv.org/abs/2212.04655" target="_blank"><img class="imgbadge arxivbadge"></a>
              </td></tr>
            </table>
          </div> 

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/NIPS2022_nerfs3.gif">
                </td>
                <td>
                  <p class="title">S<sup>3</sup>-NeRF: Neural Reflectance Field from Shading and Shadow under a Single Viewpoint</p>
                </td>
              </tr>
              <tr><td>
         		    <p class="authors">Wenqi Yang, Guanying Chen, <span class="author_me">Chaofeng Chen</span>, Zhenfang Chen, Kwan-Yee K. Wong</p>
              </td></tr>
              <tr><td>
                <p class="venue">Conference on Neural Information Processing Systems (NeurIPS), 2022</p>
              </td></tr>
              <tr><td>
                <a href="https://arxiv.org/abs/2210.08936" target="_blank"><img class="imgbadge arxivbadge"></a>
                <a href="https://github.com/ywq/s3nerf" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/ywq/s3nerf?style=social"></a>
                <a href="https://ywq.github.io/s3nerf" target="_blank"><img class="prjbadge" src="https://img.shields.io/badge/project-S3--NeRF-e9f1f6?style=flat-square"></a>
              </td></tr>
            </table>
          </div> 
          
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/ECCV2022_fastvqa.gif">
                </td>
                <td>
                  <p class="title">FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling</p>
                </td>
              </tr>
              <tr><td>
         		    <p class="authors">Haoning Wu, <span class="author_me">Chaofeng Chen</span>, Jingwen Hou, Liang Liao, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin</p>
              </td></tr>
              <tr><td>
                <p class="venue">European Conference on Computer Vision (ECCV), 2022</p>
              </td></tr>
              <tr><td>
                <a href="https://arxiv.org/abs/2207.02595v1" target="_blank"><img class="imgbadge arxivbadge"></a>
                <a href="https://github.com/TimothyHTimothy/FAST-VQA" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/TimothyHTimothy/FAST-VQA?style=social"></a>
              </td></tr>
            </table>
          </div>  
        
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/ECCV2022_ReDegNet.jpg">
                </td>
                <td>
                  <p class="title">From Face to Natural Image: Learning Real Degradation for Blind Image Super-Resolution</p>
                </td>
              </tr>
              <tr><td>
         		     <p class="authors">Xiaoming Li, <span class="author_me">Chaofeng Chen</span>, Xianhui Lin, Wangmeng Zuo, Lei Zhang</p>
              </td></tr>
              <tr><td>
                 <p class="venue">European Conference on Computer Vision (ECCV), 2022</p>
              </td></tr>
              <tr><td>
                 <a href="https://arxiv.org/abs/2210.00752" target="_blank"><img class="imgbadge arxivbadge"></a>
                 <a href="https://github.com/csxmli2016/ReDegNet" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/csxmli2016/ReDegNet?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/eccv22_mvps2.gif">
                </td>
                <td>
                  <p class="title">PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo</p>
                </td>
              </tr>
              <tr><td>
         		      <p class="authors">Wenqi Yang, Guanying Chen, <span class="author_me">Chaofeng Chen</span>, Zhenfang Chen, Kwan-Yee K. Wong</p>
              </td></tr>
              <tr><td>
                  <p class="venue">European Conference on Computer Vision (ECCV), 2022</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2207.11406" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/ywq/psnerf" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/ywq/psnerf?style=social"></a>
                  <a href="https://ywq.github.io/psnerf" target="_blank"><img class="prjbadge" src="https://img.shields.io/badge/project-PS--NeRF-e9f1f6?style=flat-square"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/MM2022_FeMaSR.jpg">
                </td>
                <td>
                  <p class="title">Real-World Blind Super-Resolution via Feature Matching with Implicit High-Resolution Priors</p>
                </td>
              </tr>
              <tr><td>
         		      <p class="authors"><span class="author_me">Chaofeng Chen<sup>*</sup></span>, Xinyu Shi<sup>*</sup>, Yipeng Qin, Xiaoming Li, Tao Yang, Xiaoguang Han, Shihui Guo</p>
              </td></tr>
              <tr><td>
                  <p class="venue">ACM Multimedia, 2022 (<b style="color:red;">Oral Presentation</b>)</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2202.13142" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/chaofengc/FeMaSR" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/chaofengc/FeMaSR?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/MM2022_TPQI.jpg">
                </td>
                <td>
                  <p class="title">Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment</p>
                </td>
              </tr>
              <tr><td>
         		      <p class="authors">Liang Liao, Kangmin Xu, Haoning Wu, <span class="author_me">Chaofeng Chen</span>, Wenxiu Sun, Qiong Yan, Weisi Lin</p>
              </td></tr>
              <tr><td>
                  <p class="venue">ACM Multimedia, 2022 (<b style="color:red;">Oral Presentation</b>)</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2207.03723" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/UoLMM/TPQI-VQA" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/UoLMM/TPQI-VQA?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
         		      <img src="images/ICIP2022_ffrnet.jpg">
                </td>
                <td>
                  <p class="title">A Unified Framework for Masked and Mask-Free Face Recognition via Feature Rectification</p>
                </td>
              </tr>
              <tr><td>
         		      <p class="authors">Shaozhe Hao, <span class="author_me">Chaofeng Chen</span>, Zhenfang Chen, Kwan-Yee K. Wong</p>
              </td></tr>
              <tr><td>
                  <p class="venue">ICIP, 2022</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2202.07358" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/haoosz/FFR-Net" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/haoosz/FFR-Net?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
                  <img src="images/ICCV2021_HDRNet.jpg">
                </td>
                <td>
                  <p class="title">HDR Video Reconstruction: A Coarse-to-fine Network and A Real-world Benchmark Dataset</p>
                </td>
              </tr>
              <tr><td>
                  <p class="authors">Guanying Chen, <span class="author_me">Chaofeng Chen</span>, Shi Guo, Zhetong Liang, K.-Y. K. Wong, Lei Zhang.</p>
              </td></tr>
              <tr><td>
                  <p class="venue">International Conference on Computer Vision (ICCV), 2021</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2103.14943" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/guanyingc/DeepHDRVideo" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/guanyingc/DeepHDRVideo?style=social"></a>
                  <a href="https://guanyingc.github.io/DeepHDRVideo/" target="_blank"><img class="prjbadge" src="https://img.shields.io/badge/project-HDRNet-e9f1f6?style=flat-square"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
                  <img src="images/PSFR-GAN.jpg">
                </td>
                <td>
                  <p class="title">Progressive Semantic-Aware Style Transformation for Blind Face Restoration</p>
                </td>
              </tr>
              <tr><td>
                  <p class="authors"><span class="author_me">Chaofeng Chen</span>, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, K.-Y. K. Wong.</p>
              </td></tr>
              <tr><td>
                  <p class="venue">Computer Vision and Pattern Recognition (CVPR), 2021</p> 
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2009.08709" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/chaofengc/PSFRGAN" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/chaofengc/PSFRGAN?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          		    <img src="images/ECCV2020_dfdnet.jpg">
                </td>
                <td>
                  <p class="title">Blind Face Restoration via Deep Multi-scale Component Dictionaries</p>
                </td>
              </tr>
              <tr><td>
          		    <p class="authors">Xiaoming Li, <span class="author_me">Chaofeng Chen</span>, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, Lei Zhang</p>
              </td></tr>
              <tr><td>
                  <p class="venue">European Conference on Computer Vision (ECCV), 2020</p> 
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2008.00418" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/csxmli2016/DFDNet" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/csxmli2016/DFDNet?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          		    <img src="images/ACCV2018_face_sketch_wild.png">
                </td>
                <td>
                  <p class="title">Semi-Supervised Learning for Face Sketch Synthesis in the Wild</p>
                </td>
              </tr>
              <tr><td>
          		    <p class="authors"><span class="author_me">Chaofeng Chen</span>, Wei Liu, Xiao Tan, K.-Y. K. Wong.</p>
              </td></tr>
              <tr><td>
                  <p class="venue">Asia Conference on Computer Vision (ACCV), 2018</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/1812.04929" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/chaofengc/Face-Sketch-Wild" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/chaofengc/Face-Sketch-Wild?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          	      <img src="images/ACCV2018_scale_aware_ocr.png">
                </td>
                <td>
         		      <p class="title">SAFE: Scale Aware Feature Encoder for Scene Text Recognition</p>
                </td>
              </tr>
              <tr><td>
          		    <p class="authors">Wei Liu, <span class="author_me">Chaofeng Chen</span>, K.-Y. K. Wong.</p>
              </td></tr>
              <tr><td>
          		    <p class="venue">Asia Conference on Computer Vision (ACCV), 2018</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/1901.05770" target="_blank"><img class="imgbadge arxivbadge"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          		    <img src="images/WACV2018_face_sketch_pcf.png">
                </td>
                <td>
         		      <p class="title">Face Sketch Synthesis with Style Transfer using Pyramid Column Feature.</p>
                </td>
              </tr>
              <tr><td>
          		    <p class="authors"><span class="author_me">Chaofeng Chen<sup>*</sup></span>, Xiao Tan<sup>*</sup>, K.-Y. K. Wong. (<sup>*</sup> indicates equal contribution.)</p>
              </td></tr>
              <tr><td>
          		    <p class="venue">IEEE Winter Conference on Applications of Computer Vision (WACV), 2018</p> 
              </td></tr>
              <tr><td>
                 <a href="https://arxiv.org/abs/2009.08679" target="_blank"><img class="imgbadge arxivbadge"></a>
                 <a href="https://github.com/chaofengc/Face-Sketch" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/chaofengc/Face-Sketch?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
        		      <img src="images/AAAI2018_char_net.png">
                </td>
                <td>
				          <p class="title">Char-Net: A Character-Aware Neural Network for Distorted Scene Text Recognition</p>
                </td>
              </tr>
              <tr><td>
				          <p class="authors">Wei Liu, <span class="author_me">Chaofeng Chen</span>, K.-Y. K. Wong </p>
              </td></tr>
              <tr><td>
				          <p class="venue">AAAI Conference on Artificial Intelligence (AAAI), 2018 (<b style="color:red;">Oral Presentation</b>)</p> 
              </td></tr>
              <tr><td>
				          <a class="pdflink"><a href="http://www.visionlab.cs.hku.hk/publications/wliu_aaai18.pdf">PDF</a>
                  <a class="pptlink" href="http://www.visionlab.cs.hku.hk/publications/wliu_aaai18.pptx">PPT</a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
				         <img src="images/BMVC2016_star_net.png">
                </td>
                <td>
				         <p class="title">STAR-Net: A SpaTial Attention Residue Network for Scene Text Recognition.</p>
                </td>
              </tr>
              <tr><td>
				         <p class="authors">Wei Liu, <span class="author_me">Chaofeng Chen</span>, K.-Y. K. Wong, Z. Su and J. Han</p>
              </td></tr>
              <tr><td>
				         <p class="venue">British Machine Vision Conference (BMVC), 2016</p>
              </td></tr>
              <tr><td>
				         <a class="pdflink" href="http://www.visionlab.cs.hku.hk/publications/wliu_bmvc16.pdf">PDF</a>
              </td></tr>
            </table>
          </div>  

     <!-- </div> -->

    <!-------------------------------  Journals -------------------------------------------->
      <h3>Academic Journals</h3> 
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          		    <img src="images/TPAMI2023_FastVQA.jpg">
                </td>
                <td>
                  <p class="title">Neighbourhood Representative Sampling for Efficient End-to-end Video Quality Assessment.</p>
                </td>
              </tr>
              <tr><td>
          		  <p class="authors">Haoning Wu, <span class="author_me">Chaofeng Chen</span>, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, Jinwei Gu, Weisi Lin</p>
              </td></tr>
              <tr><td>
                <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023</p> 
              </td></tr>
              <tr><td>
                <a href="https://arxiv.org/abs/2210.05357" target="_blank"><img class="imgbadge arxivbadge"></a>
                <a href="https://github.com/timothyhtimothy/FAST-VQA-and-FasterVQA" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/VQAssessment/FAST-VQA-and-FasterVQA?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd"><img src="images/CVIU2023_FaceSCG.png"></td>
                <td><p class="title">Semi-supervised Cycle-GAN for face photo-sketch translation in the wild</p></td>
              </tr>
              <tr>
          		  <td><p class="authors"><span class="author_me">Chaofeng Chen</span>, Wei Liu, Xiao Tan, Kwan-Yee K. Wong</p></td>
              </tr>
              <tr>
                <td><p class="venue">Computer Vision and Image Understanding (CVIU), 2023. <i>(ACCV extension)</i></p></td> 
              </tr>
              <tr><td>
                <a href="https://doi.org/10.1016/j.cviu.2023.103775" target="_blank">[DOI]</a>
                <a href="https://arxiv.org/abs/2307.10281" target="_blank"><img class="imgbadge arxivbadge"></a>
                <a href="https://github.com/chaofengc/Face-Sketch-SCG" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/chaofengc/Face-Sketch-SCG?style=social"></a>
              </td></tr>
            </table>
        	</div> 

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          		    <img src="images/TCSVT2023_DisCoVQA.png">
                </td>
                <td>
                  <p class="title">DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment</p>
                </td>
              </tr>
              <tr><td>
          		  <p class="authors">Haoning Wu, <span class="author_me">Chaofeng Chen</span>, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, Weisi Lin</p>
              </td></tr>
              <tr><td>
                <p class="venue">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2023</p> 
              </td></tr>
              <tr><td>
                <a href="https://arxiv.org/abs/2206.09853" target="_blank"><img class="imgbadge arxivbadge"></a>
                <a href="https://github.com/QualityAssessment/DisCoVQA" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/qualityassessment/discovqa?style=social"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          		    <img src="images/TIP2021_FaceVideoInpaint.png">
                </td>
                <td>
                  <p class="title">Deep Face Video Inpainting via UV Mapping</p>
                </td>
              </tr>
              <tr><td>
          		    <p class="authors">Wenqi Yang, Zhenfang Chen, <span class="author_me">Chaofeng Chen</span>, Guanying Chen, Kwan-Yee K. Wong</p>
              </td></tr>
              <tr><td>
                  <p class="venue">IEEE Transactions on Image Processing (TIP), 2023</p> 
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2109.00681" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://ywq.github.io/FVIP/" target="_blank"><img class="prjbadge" src="https://img.shields.io/badge/project-FVIP-e9f1f6?style=flat-square"></a>
              </td></tr>
            </table>
          </div>  

          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" class="imgtd">
          		    <img src="images/TIP2020_SPARNet.png">
                </td>
                <td>
                  <p class="title">Learning Spatial Attention for Face Super-Resolution</p>
                </td>
              </tr>
              <tr><td>
          		    <p class="authors"><span class="author_me">Chaofeng Chen</span>, Dihong Gong, Hao Wang, Zhifeng Li, Kwan-Yee K. Wong</p>
              </td></tr>
              <tr><td>
                  <p class="venue">IEEE Transactions on Image Processing (TIP), 2020</p>
              </td></tr>
              <tr><td>
                  <a href="https://arxiv.org/abs/2012.01211" target="_blank"><img class="imgbadge arxivbadge"></a>
                  <a href="https://github.com/chaofengc/Face-SPARNet" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/chaofengc/Face-SPARNet?style=social"></a>
              </td></tr>
            </table>
          </div>  

      <h2>PhD Dissertation</h2>
          <div class="hflex-container" id="paper">
            <table>
              <tr>
                <td rowspan="4" width="200px">
          	      <img src="images/Logo_HKU.jpg">
                </td>
                <td>
                  <p class="title">Face Sketch Synthesis and Face Super Resolution in the Wild with Deep Learning</p>
                </td>
              </tr>
              <tr><td>
          		    <p class="authors"><span class="author_me">Chaofeng Chen</span></p>
              </td></tr>
              <tr><td>
                  <p class="venue">Dept. of Computer Science, The University of Hong Kong, 2020</p>
              </td></tr>
              <tr><td>
                  <a class="pdflink" href="https://hub.hku.hk/handle/10722/297490" target="_blank">HKU Theses Online</a>
              </td></tr>
            </table>
          </div>  

    <!-------------------------------  Other information -------------------------------------------->
      <h2>Professional Activities</h2>
			<ul>
        <li><b>Conference Reviewer</b>:
          <ul>
            <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
            <li>International Conference on Computer Vision (ICCV)</li>
            <li>European Conference on Computer Vision (ECCV)</li>
            <li>Association for the Advancement of Artificial Intelligence (AAAI)</li>
            <li>ACM International Conference on Multimedia (ACM MM)</li>
          </ul>
        </li>
        <br>
				<li><b>Journal Reviewer</b>:
          <ul>
            <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
            <li>IEEE Transactions on Image Processing (TIP)</li>
            <li>IEEE Transactions on Multimedia (TMM)</li>
            <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
            <li>Elsevier Journal of Neurocomputing (Neurocomputing)</li>
          </ul>
			</ul>

      <h2>Awards</h2>
			<ul>
        <li>Hong Kong PhD Fellowship, HKU, 2015 - 2018</li>
				<li>National Scholarship, HUST, 2011 - 2012</li>
			</ul>

		  <!-- <div class="content-container"> -->
			<h2>Teaching</h2>
			<ul>
        <li>[2017/18 2nd semester]: COMP3317 Computer Vision&nbsp;~ Teaching Assistant</li>
				<li>[2015/16 2nd semester]: COMP2396 Object-Oriented Programming and Java&nbsp;~ Teaching Assistant</li>
				<li>[2016/17 1st semester]: COMP2396 Object-Oriented Programming and Java&nbsp;~ Teaching Assistant</li>
			</ul>

			<h2>Friend Links</h2>
			<table width="800">
				<tr>
					<td>
					    <ul>
					       	<li><a href="http://xiaolongzhu.org" target="_blank">Xiaolong Zhu</a></li>
					       	<li><a href="http://www.xtan.org" target="_blank">Xiao Tan</a></li>
					       	<li><a href="http://www.hankai.org" target="_blank">Kai Han</a></li>
					       	<li><a href="http://www.cs.hku.hk/~wliu" target="_blank">Wei Liu</a></li>
					       	<li><a href="https://guanyingc.github.io/" target="_blank">Guanying Chen</a></li>
					       	<li><a href="https://zfchenunique.github.io/" target="_blank">Zhenfang Chen</a></li>
					       	<li><a href="https://csxmli2016.github.io//" target="_blank">Xiaoming Li</a></li>
						</ul>
					</td>
					<td>
						<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=8L5p&d=oIqbE94OTEQX6b36vXK28iFbZ1SwGCJXKVMwX5205rQ"></script>
					</td>
				</tr>
			</table>
		<!-- </div> -->
	</div>
</body>
</html>
